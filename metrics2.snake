

import os
import csv
from datetime import datetime
from collections import defaultdict

with open(config.get('project',{}).get('sample_list','samples.list'),'r') as i:
    SAMPLES=i.read().splitlines()

with open(config['project']['bam_list'],'r') as b:
    BAMS=dict(line.split('\t') for line in b.read().splitlines())

def sample_bam(wildcards):
    return BAMS[wildcards.sample]

header='SAMPLE BAIT_SET TOTAL_READS PCT_PF_READS PF_HQ_ALIGNED_READS PCT_READS_ALIGNED_IN_PAIRS'.split(' ')
if config['resources'].get('disambiguate',False):
    header+=['PCT_HUMAN','PCT_MOUSE','PCT_AMBIGUOUS']
header+=['PCT_DUP']
header+='PCT_SELECTED_BASES MEAN_TARGET_COVERAGE MEDIAN_TARGET_COVERAGE MAX_TARGET_COVERAGE PCT_USABLE_BASES_ON_TARGET ZERO_CVG_TARGETS_PCT PCT_TARGET_BASES_10X PCT_TARGET_BASES_40X PCT_TARGET_BASES_100X'.split(' ')

localrules:Summary,all_amplicon

rule standard_summary:
    input:
        expand('{project}.{date}.metrics_summary.txt',project=config['project']['name'],date=datetime.today().strftime('%Y%m%d'))

rule target_coverage_summary:
    input:
        expand('{project}.{date}.mean_target_coverage.txt',project=config['project']['name'],date=datetime.today().strftime('%Y%m%d'))

rule all_amplicon:
    input:
        expand('bam_input/final/{sample}/metrics/{reference}/amplicon.metrics',sample=SAMPLES,reference=config['reference']['key']),
        expand("bam_input/final/{sample}/metrics/{reference}/insert_size.metrics",sample=SAMPLES,reference=config['reference']['key']),
        expand("bam_input/final/{sample}/metrics/{reference}/target.metrics",sample=SAMPLES,reference=config['reference']['key'])

###############################################

rule CollectAlignmentSummaryMetrics:
    input:
        sample_bam
    output:
        "bam_input/final/{sample}/metrics/{reference}/alignment_summary.metrics"
    params:
        reference=config['reference']['fasta'],
        memory="10240m"
    shell:
        "java -Xmx{params.memory} -jar $HOME/software/picard/2.20.2/picard.jar CollectAlignmentSummaryMetrics R={params.reference} I={input} O={output} VALIDATION_STRINGENCY=SILENT"

rule CollectInsertSizeMetrics:
    input:
        sample_bam
    output:
        "bam_input/final/{sample}/metrics/{reference}/insert_size.metrics",
        "bam_input/final/{sample}/metrics/{reference}/insert_size_histogram.pdf"
    params:
        reference=config['reference']['fasta'],
        memory="10240m",
        MINIMUM_PCT=0.5
        #Default value = 0.05. but documentation states "If processing a small file, set the minimum percentage option (M) to 0.5, otherwise an error may occur."
    shell:
        "java -Xmx{params.memory} -jar $HOME/software/picard/2.20.2/picard.jar CollectInsertSizeMetrics R={params.reference} I={input} O={output[0]} H={output[1]} M={params.MINIMUM_PCT} VALIDATION_STRINGENCY=SILENT"

rule CollectHsMetrics:
    input:
        sample_bam
    output:
        "bam_input/final/{sample}/metrics/{reference}/target.metrics",
        "bam_input/final/{sample}/metrics/{reference}/target_coverage.metrics"
    params:
        reference=config['reference']['fasta'],
        baits=config['resources']['picard_intervals'],
        targets=config['resources']['picard_intervals'],
        memory="10240m"
    wildcard_constraints:
        target=config['resources']['targets_key']
    shell:
        "java -Xmx{params.memory} -jar $HOME/software/picard/2.20.2/picard.jar CollectHsMetrics R={params.reference} I={input} O={output[0]} BAIT_INTERVALS={params.baits} TARGET_INTERVALS={params.targets} PER_TARGET_COVERAGE={output[1]} VALIDATION_STRINGENCY=SILENT"

rule CollectWgsMetrics:
    input:
        "bam_input/final/{sample}/{reference}/{sample}.ready.bam"
    output:
        "bam_input/final/{sample}/metrics/{reference}/whole_genome.metrics"
    params:
        reference=config['reference']['fasta'],
        memory="10240m"
    shell:
        "java -Xmx{params.memory} -jar $HOME/software/picard/2.20.2/picard.jar CollectWgsMetrics R={params.reference} I={input} O={output}"

rule Summary:
    input:
        expand("bam_input/final/{sample}/metrics/{reference}/alignment_summary.metrics",sample=SAMPLES,reference=config['reference']['key']),
        expand("bam_input/final/{sample}/metrics/{reference}/target.metrics",sample=SAMPLES,reference=config['reference']['key'])
    output:
        expand('{project}.{date}.metrics_summary.txt',project=config['project']['name'],date=datetime.today().strftime('%Y%m%d'))
    run:
        ref=config['reference']['key']
        with open(output[0],'w') as ofile:
            writer=csv.DictWriter(ofile,delimiter='\t',fieldnames=header)
            writer.writeheader()
            for sample in SAMPLES:
                some_dict=defaultdict(str)
                if os.path.isfile('bam_input/final/{0}/metrics/{1}/mark_duplicates.table'.format(sample,ref)):
                    #put these in defs?
                    with open('bam_input/final/{0}/metrics/{1}/mark_duplicates.table'.format(sample,ref),'r') as file:
                        for line in file:
                            if line.startswith('LIBRARY'):
                                values=dict(zip(line.rstrip().split('\t'),file.__next__().rstrip().split('\t')))
                                break
                        some_dict['PCT_DUP']=values['PERCENT_DUPLICATION']
                with open('bam_input/final/{0}/metrics/{1}/alignment_summary.metrics'.format(sample,ref),'r') as aln_f:
                    for line in aln_f:
                        if line.startswith('CATEGORY'):
                            break
                    reader=csv.DictReader(aln_f,delimiter='\t',fieldnames=line.rstrip().split('\t'))
                    FIRST_OF_PAIR=reader.__next__()
                    SECOND_OF_PAIR=reader.__next__()
                    PAIR=reader.__next__()
                    for k,v in PAIR.items():
                        if k in header:
                            some_dict[k]=v
                with open('bam_input/final/{0}/metrics/{1}/target.metrics'.format(sample,ref),'r') as file:
                    for line in file:
                        if line.startswith('BAIT_SET'):
                            break
                    reader=csv.DictReader(file,delimiter='\t',fieldnames=line.rstrip().split('\t'))
                    values=dict(zip(line.rstrip().split('\t'),file.__next__().rstrip().split('\t')))
                    for k,v in values.items():
                        if k in header:
                            some_dict[k]=v
                disambres=False
                if os.path.isfile('bam_input/work/{0}/GRCh37/disambres/input_summary.txt'.format(sample)):
                    disambres='bam_input/work/{0}/GRCh37/disambres/input_summary.txt'.format(sample)
                elif os.path.isfile('bam_input/work/{0}/GRCh37/disambres/mapped_summary.txt'.format(sample)):
                    disambres='bam_input/work/{0}/GRCh37/disambres/input_summary.txt'.format(sample)
                if disambres:
                    with open(disambres,'r') as file:
                        try:
                            values=dict(zip(file.__next__().rstrip().split('\t'),file.__next__().rstrip().split('\t')))
                            total=sum(int(values.get(k,0)) for k in ['unique species A pairs','unique species B pairs','ambiguous pairs'])
                            #['PCT_HUMAN','PCT_MOUSE','PCT_AMBIGUOUS']
                            some_dict['PCT_HUMAN']=str(int(values.get('unique species A pairs',0))/total)
                            some_dict['PCT_MOUSE']=str(int(values.get('unique species B pairs',0))/total)
                            some_dict['PCT_AMBIGUOUS']=str(int(values.get('ambiguous pairs',0))/total)
                        except IndexError:
                            pass
                some_dict['SAMPLE']=sample
                for k,v in some_dict.items():
                    if 'PCT' in k:
                        some_dict[k]=str(round(float(some_dict[k]),3))
                writer.writerow(some_dict)

rule Target_Coverage_Summary:
    input:
        expand("bam_input/final/{sample}/metrics/{reference}/target_coverage.metrics",sample=SAMPLES,reference=config['reference']['key'])
    output:
        expand('{project}.{date}.mean_target_coverage.txt',project=config['project']['name'],date=datetime.today().strftime('%Y%m%d'))
    run:
        target_coverage=defaultdict(lambda: defaultdict(float))
        sample_file=defaultdict(str)
        key_order=[]
        for sample in SAMPLES:
            sample_file[sample]=[x for x in input if sample in x][0]
            with open(sample_file[sample],'r') as cov_file:
                reader=csv.DictReader(cov_file,delimiter='\t')
                for row in reader:
                    key=tuple([row[x] for x in ['chrom','start','end','length','name']])
                    if key not in key_order:
                        key_order.append(key)
                    target_coverage[key][sample]=float(row['mean_coverage'])
        with open(output[0],'w') as outfile:
            writer=csv.writer(outfile,delimiter='\t')
            writer.writerow(['Chr','Start','End','Length','Name']+SAMPLES)
            for key in key_order:
                row=list(key)+[target_coverage[key][sample] for sample in SAMPLES]
                writer.writerow(row)